{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import string\n",
    "import random\n",
    "from data_utils import *\n",
    "from rnn import *\n",
    "import torch\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "#Set GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Load vocabulary files\n",
    "input_lang = torch.load('data-bin/fra.data')\n",
    "output_lang = torch.load('data-bin/eng.data')\n",
    "\n",
    "#Create and empty RNN model\n",
    "encoder = EncoderRNN(input_size=input_lang.n_words, device=device)\n",
    "attn_decoder = AttnDecoderRNN(output_size=output_lang.n_words, device=device)\n",
    "\n",
    "#Load the saved model weights into the RNN model\n",
    "encoder.load_state_dict(torch.load('model/encoder'))\n",
    "attn_decoder.load_state_dict(torch.load('model/decoder'))\n",
    "\n",
    "#Return the decoder output given input sentence \n",
    "#Additionally, the previous predicted word and previous decoder state can also be given as input\n",
    "def translate_single_word(encoder, decoder, sentence, decoder_input=None, decoder_hidden=None, max_length=MAX_LENGTH, device=device):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence, device)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        encoder = encoder.to(device)\n",
    "        decoder = decoder.to(device)\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        if decoder_input==None:\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        else:\n",
    "            decoder_input = torch.tensor([[output_lang.word2index[decoder_input]]], device=device) \n",
    "        \n",
    "        if decoder_hidden == None:        \n",
    "            decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        return decoder_output.data, decoder_hidden\n",
    "\n",
    "\n",
    "# It reads hardcoded sentences as input, translates using the trained RNN and saves the outputs in example.txt file.\n",
    "def translate_example():\n",
    "\ttarget_sentences = [\"i can speak a bit of french .\",\n",
    "\t\t\t\"i ve bought some cheese and milk .\",\n",
    "\t\t\t\"boy where is your older brother ?\",\n",
    "\t\t\t\"i ve just started reading this book .\",\n",
    "\t\t\t\"she loves writing poems .\"]\n",
    "\n",
    "\tsource_sentences = [\"je parle un peu francais .\",\n",
    "\t\t\t\t\"j ai achete du fromage et du lait .\",\n",
    "\t\t\t\t\"garcon ou est ton grand frere ?\",\n",
    "\t\t\t\t\"je viens justement de commencer ce livre .\",\n",
    "\t\t\t\t\"elle adore ecrire des poemes .\"]\n",
    "\n",
    "\ttarget = codecs.open('example.txt','w',encoding='utf-8')\n",
    "\n",
    "\tbeam_size = 1\n",
    "\tfor i,source_sentence in enumerate(source_sentences):\n",
    "\n",
    "\t\ttarget_sentence = normalizeString(target_sentences[i])\n",
    "\t\tinput_sentence = normalizeString(source_sentence)\n",
    "\t\t\n",
    "\t\thypothesis = beam_search(encoder, attn_decoder, input_sentence, beam_size=beam_size)\n",
    "\t\t\n",
    "\t\tprint(\"S-\"+str(i)+\": \"+input_sentence)\n",
    "\t\tprint(\"T-\"+str(i)+\": \"+target_sentence)\n",
    "\t\tprint(\"H-\"+str(i)+\": \"+hypothesis)\n",
    "\t\tprint()\n",
    "\t\ttarget.write(hypothesis+'\\n')\n",
    "\ttarget.close()    \n",
    "\n",
    "\n",
    "###################################################################################################################\n",
    "###Part 1. Write the function below to read the data/test.fra file and write the translations in test_beam_1.out###\n",
    "###################################################################################################################\n",
    "def translate_test():\n",
    "\t#TODO: Write the function below\n",
    "    # Read the test data\n",
    "    with open('data/test.fra', 'r', encoding='utf-8') as file:\n",
    "        test_sentences = file.readlines()\n",
    "\n",
    "    # Output file to save the translations\n",
    "    with codecs.open('test_beam_1.out', 'w', encoding='utf-8') as output_file:\n",
    "        # Beam size\n",
    "        beam_size = 5  # You can adjust the beam size as needed\n",
    "\n",
    "        for sentence in tqdm(test_sentences):\n",
    "            # Normalize and translate the sentence\n",
    "            input_sentence = normalizeString(sentence.strip())\n",
    "            translated_sentence = beam_search(encoder, attn_decoder, input_sentence, beam_size=beam_size)\n",
    "\n",
    "            # Write the translation to the output file\n",
    "            output_file.write(translated_sentence + '\\n')\n",
    "\n",
    "#############################################################################################\n",
    "###Part 2. Modify this function to use beam search to predict instead of greedy prediction###\n",
    "#############################################################################################\n",
    "def beam_search(encoder,decoder,input_sentence,beam_size=1,max_length=MAX_LENGTH):\n",
    "    decoded_output = []\n",
    "    \n",
    "    #Predicted the first word\n",
    "    decoder_output, decoder_hidden = translate_single_word(encoder, decoder, input_sentence, decoder_input=None, decoder_hidden=None)\n",
    "    \n",
    "    #Get the probability of all output words\n",
    "    decoder_output_probs = decoder_output.data\n",
    "    \n",
    "    #Select the id of the word with maximum probability\n",
    "    idx = torch.argmax(decoder_output_probs)\n",
    "\t\n",
    "    #Convert the predicted id to the word\n",
    "    first_word = output_lang.index2word[idx.item()]\n",
    "    \n",
    "    #Add the predicted word to the output list and also set it as the previous prediction\n",
    "    decoded_output.append(first_word)\n",
    "    previous_decoded_output = first_word\n",
    "    \n",
    "    #Loop until the maximum length\n",
    "    for i in range(max_length):\n",
    "    \n",
    "        #Predict the next word given the previous prediction and the previous decoder hidden state\n",
    "        decoder_output, decoder_hidden = translate_single_word(encoder, decoder, input_sentence, previous_decoded_output, decoder_hidden)\n",
    "        \n",
    "        #Get the probability of all output words\n",
    "        decoder_output_probs = decoder_output.data\n",
    "        \n",
    "        #Select the id of the word with maximum probability\n",
    "        idx = torch.argmax(decoder_output_probs)\n",
    "        \n",
    "        #Break if end of sentence is predicted\n",
    "        if idx.item() == EOS_token:\n",
    "            break \n",
    "            \n",
    "        #Else add the predicted word to the list\n",
    "        else:\n",
    "            #Convert the predicted id to the word\n",
    "            selected_word = output_lang.index2word[idx.item()]\n",
    "            \n",
    "            #Add the predicted word to the output list and update the previous prediction\n",
    "            decoded_output.append(selected_word)    \n",
    "            previous_decoded_output = selected_word\n",
    "            \n",
    "    #Convert list of predicted words to a sentence and detokenize \n",
    "    output_translation = \" \".join(i for i in decoded_output)\n",
    "    \n",
    "    return output_translation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
